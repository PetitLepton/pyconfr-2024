[
  {
    "objectID": "presentation.html#sencrop",
    "href": "presentation.html#sencrop",
    "title": "Python & kedropourl’agriculture de précision",
    "section": "Sencrop",
    "text": "Sencrop\n\n35000 stations météorologiques réparties sur toute l’Europe\n\n\n\n\n  \n  température de l'air & hygrométrie\n  \n  direction et vitesse du vent\n  \n  pluviométrie\n  \n  point de rosée\n\n\n\n\n\n\n\nSencrop c’est une startup de l’agtech fondée en 2016 dans le nord de la France, à Lille très exactement. Avec plus de 80 collaborateurs, Sencrop est le leader européen des stations météos connectées en couvrant l’Europe de l’ouest de plus de 35 000 capteurs.\nLe principe est simple, nos utilisateurs (qui sont agriculteurs la plupart du temps) peuvent installer une station dans leur champ. Ces stations possèdent plusieurs capteurs comme vous pouvez le voir sur l’écran juste ici. Les mesures apparaissent ensuite sur une application sur téléphone ou ordinateur. Ces données météorologiques permettent à nos utilisateurs de prendre des décisions sur leur opération quotidienne. Ainsi, nos agriculteurs réussissent à optimiser leur l’irrigation afin de gagner certains volumes d’eau, mais également optimiser leur pulvérisation de produit phytosanitaire (qui coute très cher). Du côté agronomique, nous arrivons aussi à fournir de précieux conseils sur les périodes de risques maladies (pour protéger au mieux du mildiou par exemple) ou les périodes de gel (qui touchent régulierement nos vignerons)."
  },
  {
    "objectID": "presentation.html#spatialisation",
    "href": "presentation.html#spatialisation",
    "title": "Python & kedropourl’agriculture de précision",
    "section": "Spatialisation",
    "text": "Spatialisation\n\nEst-ce que l’on peut fournir de la donnée météorologique de qualité sur n’importe quelle localisation sur le territoire ?\n\n\n\n\n\nutilisation de maillage h3 : index spatial hiérarchique\ncomparaison des mesures de stations avec les médianes sur les mailles\n\n\n\n\n\n\nAujourd’hui Sencrop est présente dans plus de 10 pays à travers l’Europe, principale autour de la France. Malgré notre large réseau de stations, nous faisons face à un défi majeur : la couverture géographique n’est pas uniforme. Certaines régions françaises, par exemple, sont moins densément équipées que d’autres. Cette situation pose un problème crucial : comment fournir des données météorologiques précises pour n’importe quel point GPS, même en l’absence de station à proximité immédiate ? C’est là qu’intervient notre solution de génération de séries temporelles. L’objectif est ambitieux : créer un système capable de produire des données météorologiques fiables pour n’importe quelle coordonnée géographique en Europe de l’Ouest.\nPour relever ce défi de spatialisation des données, nous nous appuyons sur H3, le système de géo-indexation hiérarchique développé par Uber. Ce choix n’est pas anodin. H3 divise la surface terrestre en hexagones de taille uniforme. Ce qui rend H3 particulièrement pertinent pour notre usage, c’est que :\n\nLes hexagones minimisent la distorsion des distances entre le centre et les bords, contrairement aux carrés\nLe système est hiérarchique avec 16 résolutions, allant d’hexagones de 1000km à moins d’un mètre de diamètre\nChaque hexagone a un identifiant unique, ce qui facilite l’indexation et la recherche\nLes cellules voisines sont facilement identifiables grâce à l’algorithme de H3\n\nDans notre cas, nous utilisons principalement la résolution 7, qui correspond à des hexagones d’environ 5km de diamètre. Cette échelle nous permet de :\n\nAgréger efficacement les données de nos stations au sein d’une même cellule\nCréer une grille régulière pour l’interpolation des données\nOptimiser les performances de calcul tout en maintenant une précision pertinente pour l’agriculture\n\nL’utilisation de H3 nous permet ainsi de structurer spatialement nos données météorologiques de manière efficace et d’appliquer nos algorithmes d’interpolation avec une plus grande confiance dans les résultats."
  },
  {
    "objectID": "presentation.html#kedro-en-trois-mots",
    "href": "presentation.html#kedro-en-trois-mots",
    "title": "Python & kedropourl’agriculture de précision",
    "section": "kedro en trois mots",
    "text": "kedro en trois mots\n\n\nlibrairie de transformations de données\n\ndécouplage entre les sources de données et les transformations\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    node2[node]\n    node7[node]\n    \n    input1[(dataset)]\n    input2[(dataset)]\n    output[(dataset)]\n    \n    input1 --&gt; node2\n    input2 --&gt; node2\n\n  subgraph pipeline[pipeline]\n    direction LR\n    node2 --&gt; node7\n    %% node3 --&gt; node5 --&gt; node7\n    %% input3 --&gt; node5\n  end\n\n  node7 --&gt; output\n\n  style pipeline color:#000\n\n\n\n\n\n\n\n\n\nnode\n\nfonction — au sens Python — avec un/des dataset d’entrée/sortie\n\npipeline\n\nséquence de node\n\ncatalog\n\nun ensemble de dataset\n\n\n\n\n\nkedro est une librairie open-source qui a été initialement développée par Quantum Black, la branche AI de Mc Kinsey (probablement financé par le contribuable français) qui a été depuis transmise à la Linux Foundation. Je devais être destiné à utiliser kedro parce que, comme les trous noirs quantiques, je n’ai pas de cheveux.\nkedro permet de construire efficacement des pipelines de transformations de données. Pour celleeux qui ne seraient pas familières avec la notion de pipeline, le schéma de gauche en présente un pipelines simplifié. Le pipeline est ici très simple, il contient deux noeuds séquentiels qui sont alimentés en amont par deux jeux de données et qui résultent en la création d’un troisième.\nkedro reprend, mot pour mot, ces trois concepts: des nœuds, des pipelines et un catalogue dont nous allons expliquer la signification dans quelques instants."
  },
  {
    "objectID": "presentation.html#pipelines-nodes",
    "href": "presentation.html#pipelines-nodes",
    "title": "Python & kedropourl’agriculture de précision",
    "section": "pipelines & nodes",
    "text": "pipelines & nodes\n\n\n\n\n\n\n\nflowchart TB\n    node2[map_locations_to_grids]\n    node7[extract_unique_grid_ids]\n    \n    input1[(locations)]\n    input2[h3-grid-resolution]\n    output[(unique-grid-ids)]\n    \n  subgraph pipeline[map_locations_to_grids]\n    direction TB\n    node2 --&gt; node7\n    %% node3 --&gt; node5 --&gt; node7\n    input1 --&gt; node2\n    input2 --&gt; node2\n    %% input3 --&gt; node5\n    node7 --&gt; output\n  end\n\n  style pipeline color:#000\n\n\n\n\n\n\n\n\n\n\n\n/src/pipelines/map_locations_to_grids/pipeline.py\n\nfrom typing import Any\nfrom kedro.pipeline import Pipeline, node\nfrom .nodes import map_locations_to_grids, extract_unique_grid_ids\n\n\ndef create_pipeline(**kwargs: Any) -&gt; Pipeline:\n    return Pipeline(\n        nodes=[\n            node(\n                map_locations_to_grids,\n                inputs={\n                    \"locations\": \"locations\",\n                    \"resolution\": \"params:h3-grid-resolution\",\n                },\n                outputs=\"grids\",\n            ),\n            node(\n                extract_unique_grid_ids,\n                inputs={\"grids\": \"grids\"},\n                outputs=\"unique-grid-ids\",\n            )\n        ],\n    )\n\n\n\n\n\n/src/pipelines/map_locations_to_grids/nodes.py\n\nimport h3\nimport pandas\n\n\ndef map_locations_to_grids(\n    locations: pandas.DataFrame, resolution: int\n) -&gt; pandas.DataFrame:\n    return pandas.DataFrame(\n        data=[\n            {\n                \"location_id\": location[\"id\"],\n                \"grid_id\": h3.geo_to_h3(\n                    lat=location[\"latitude\"],\n                    lng=location[\"longitude\"],\n                    resolution=resolution,\n                ),\n            }\n            for location in locations.to_dict(orient=\"records\")\n        ]\n    )\n\ndef extract_unique_grid_ids(grids: pandas.DataFrame) -&gt; pandas.DataFrame:\n    return grids[\"grid_id\"].unique()\n\n\n\n\n\nUn pipeline est une class Python constituée de séquences de node. La structure du DAG, ce qui en fait un pipeline, ce sont les entrées/sorties.\nSi la sortie d’un nœud fait partie des entrées d’un autre alors ces deux nœuds seront exécutés séquentiellement. Dans l’exemple à gauche, le nœud map_locations_to_grids intervient avant extract_unique_grid_ids parce que sa sortie grids est le jeu de donnée d’entrée d’extract_unique_grid_ids.\nUn nœud au sens de kedro juste une fonction python qui peut avoir des entrées et des sorties. Elle est totalement indépendante du reste et peut/doit donc être testée de manière unitaire.\nVous remarquerez que les entrées/sorties sont définies par des chaînes de caractères, des noms de datasets. Que représentent-ils et comment passe-t-on d’une chaîne de caractère à un DataFrame pandas comme dans l’exemple. Et bien, c’est le catalogue !"
  },
  {
    "objectID": "presentation.html#catalog-environment",
    "href": "presentation.html#catalog-environment",
    "title": "Python & kedropourl’agriculture de précision",
    "section": "catalog & environment",
    "text": "catalog & environment\n\ncatalog : définition des dataset d’entrée et de sortie\nenvironment : ensemble du catalog et d’un jeu de paramètres\n\n\n\n\n\n/conf/test-local/catalog.yaml\n\nlocations:\n  type: pandas.JSONDataset\n  filepath: data/01_raw/test_local/two_locations.json\n\ngrids:\n  type: pandas.JSONDataset\n  filepath: data/02_intermediate/grids.json\n\n\nkedro run --pipeline map_locations_to_grids --env test-local\n\n\n\n\n/conf/production/catalog.yaml\n\nlocations:\n  type: pandas.JSONDataset\n  filepath: s3://virtual-stations/locations.json\n\n\n\n\n\n\n\nkedro run --pipeline map_locations_to_grids --env production\n\n\n\nLe catalogue est une liste de tous les datasets qui sont matérialisés — c’est à dire pas seulement en mémoire. Il se présente sous la forme d’un fichier yaml dont chaque clef principale correspond aux noms que vous trouvez dans les pipelines. L’élément le plus important est le type, c’est lui va déterminer comment on passe d’un nom à une structure python.\nDans notre exemple, la dataset est un pandas.JSONDataset. C’est une classe qui utilise pandas pour lire un fichier JSON, qui ici, se trouve localement dans un dossier. Le dataset est en charge de lire le fichier JSON et de trasnformer le résultat en un DataFrame pandas.\nVous remarquerez que le catalogue es défini dans un dossier conf/test-local. Pour exécuter le pipeline, vous utilisez la CLI de kedro en précisant l’environnement, c’est-à-dire le nom du sous-dossier, ici test-local. Et c’est ici que réside l’un des forces les plus tangibles de kedro. Vous avez remarqué que le pipeline précédent est indépendant des sources de données, la seule chose importante est que le dataset renvoie dans notre cas un dataframe. Je peux donc définir un autre catalogue avec un autre type de dataset qui va lui aussi me fournir une dataframe. Par exemple, je peux créer un catalogue avec un fichier JSON dans un bucket s3 et jouer mon pipeline avec ce nouvel environnement !\nNotez que tous ces datasets existent déjà, c’est une librairie appelée kedro-datasets qui est maintenue par les équipes de quantum black et la communauté."
  },
  {
    "objectID": "presentation.html#structure-du-projet",
    "href": "presentation.html#structure-du-projet",
    "title": "Python & kedropourl’agriculture de précision",
    "section": "structure du projet",
    "text": "structure du projet\n\nséparation des sources de données et des pipelines\n\n.\n├── conf\n│   ├── base\n│   │   ├── catalog.yml\n│   │   └── parameters.yml\n│   └── local\n│       └── credentials.yml\n├── data\n│   ├── 01_raw\n│   └── ...\n├── notebooks\n├── pyproject.toml\n└── src\n    └── my_project\n        ├── pipeline_registry.py\n        └── pipelines\n            └── my_pipeline\n                ├── nodes.py\n                └── pipeline.py\n\nVous l’avez compris, kedro impose une séparation claire des datasets et des pipelines, des sources/sink et des transformations. Cela se retrouve dans la structure d’un projet. Tous les catalogues se trouvent dans le dossier conf. Dans l’exemple précédent, j’aurais créé le dossier test-local.\nLes pipelines sont dans le dossier pipelines et sont répertoriés dans le fichier registry.\nL’avantage de cette structure fixe est qu’elle permet à une personne qui connaît un peu kedro de voyager dans le projet facilement."
  },
  {
    "objectID": "presentation.html#dumas-du-tuyau-trois-écrivains-et-un-lecteur",
    "href": "presentation.html#dumas-du-tuyau-trois-écrivains-et-un-lecteur",
    "title": "Python & kedropourl’agriculture de précision",
    "section": "Dumas du tuyau : trois écrivains et un lecteur",
    "text": "Dumas du tuyau : trois écrivains et un lecteur\n\n\n\n\n\n\nflowchart LR\n\n    measures[(&lt;br&gt;mesures&lt;br&gt;des stations)]\n    getstatistics[&lt;br&gt;calculs&lt;br&gt;des statistiques]\n\n    meteoblue[(&lt;br&gt;meteoblue&lt;br&gt;API)]\n  getforecasts[&lt;br&gt;récupération&lt;br&gt;des prévisions]\n  gethistory[&lt;br&gt;récupération&lt;br&gt;de l'historique]\n\n    forecasts[(&lt;br&gt;prévisions)]\n    history[(&lt;br&gt;historique)]\n    statistics[(&lt;br&gt;statistiques)]\n\n    measures --&gt; getstatistics --&gt; statistics\n    meteoblue --&gt; getforecasts --&gt; forecasts\n    meteoblue --&gt; gethistory --&gt; history\n    \n    combine[&lt;br&gt;combinaison&lt;br&gt;des sources] --&gt; timeseries[(&lt;br&gt;série temporelle&lt;br&gt;finale)]\n  statistics --&gt; combine\n    forecasts --&gt; combine\n    history --&gt; combine\n\n\n\n\n\n\n\n\nPour générer des séries temporelles précises pour n’importe quel point GPS, nous avons mis en place une architecture qui combine plusieurs sources de données. Laissez-moi vous expliquer notre pipeline de données. Tout d’abord, nous exploitons deux sources principales :\nNos stations Sencrop :\nNous collectons en temps réel les mesures de nos stations déployées sur le terrain Ces données brutes sont traitées pour calculer des statistiques pertinentes (principalement des médianes)\nL’API Meteoblue : Cette source externe nous fournit deux types de données :\nDes prévisions météorologiques pour compléter nos données en temps réel Un historique météorologique pour le passé\nL’étape cruciale de notre système est la combinaison de ces sources. Notre algorithme fusionne intelligemment :\nLes statistiques issues de nos stations Les prévisions Meteoblue L’historique Meteoblue\nCette approche multi-sources nous permet de générer une série temporelle finale qui bénéficie :\nDe la précision de nos mesures terrain là où nous avons des stations De la couverture géographique complète offerte par Meteoblue D’une continuité temporelle entre passé, présent et futur\nLe résultat est une série temporelle cohérente et fiable, disponible pour n’importe quel point géographique de notre zone de couverture.\n\n\nPassage sur Kedro-viz Laissez-moi maintenant vous présenter l’architecture de notre pipeline de données, visualisée ici grâce à Kedro-viz.  Pour ceux qui ne connaissent pas, Kedro-viz est un outil fantastique qui nous permet de visualiser nos pipelines de données de manière claire et interactive, ce qui rend la compréhension de systèmes complexes beaucoup plus accessible.\nDans notre pipeline, tout commence avec les ‘Locations’ - ce sont les points GPS pour lesquels nous voulons générer des séries temporelles. Ces coordonnées constituent l’entrée principale de notre système. Ces points GPS sont ensuite convertis en ‘Grids’ - il s’agit des fameuses cellules hexagonales H3 dont nous parlions précédemment. Cette étape de conversion est cruciale car elle nous permet de passer d’une logique de points GPS arbitraires à une structure spatiale régulière et optimisée. Le pipeline se complexifie ensuite avec plusieurs étapes de traitement :\nLa récupération des séries temporelles sur les cellules H3 pour les différentes sources.\nCes traitements convergent vers la production de séries temporelles ordonnées (‘Ordered Time Series On Grids’), qui sont ensuite affinées pour produire les séries temporelles finales. Le point d’orgue de ce pipeline est la dernière étape : ‘Time Series On Locations’. C’est ici que nous revenons à notre objectif initial - fournir des données météorologiques précises pour chaque point GPS demandé. Pour chaque localisation, nous stockons une série temporelle complète, combinant ainsi la puissance de notre infrastructure de données avec la précision géographique dont nos utilisateurs ont besoin."
  },
  {
    "objectID": "presentation.html#kafka-sur-le-rivage",
    "href": "presentation.html#kafka-sur-le-rivage",
    "title": "Python & kedropourl’agriculture de précision",
    "section": "Kafka sur le rivage",
    "text": "Kafka sur le rivage\n\nkafka n’est pas le sujet mais c’est notre dataset de production !\n\n\n\n\n\n/conf/test-measures/catalog.yml\n\nlocations:\n  type: pandas.JSONDataset\n  filepath: data/01_raw/test_measures/locations.json\n\nformatted_measures_on_grids:\n  type: pandas.CSVDataset\n  filepath: data/03_primary/test_measures/measures_on_grids.csv\n\n  \n\n\nkedro run --pipeline fetch_measures_on_grids --env test-measures\n\n\n\n/conf/production/catalog.yml\n\nlocations:\n  type: pandas.JSONDataset\n  filepath: s3://virtual-stations/virtual-stations.json\n\nformatted_measures_on_grids:\n  type: datasets.ConfluentKafkaAvroDataset\n  save_args:\n    topic: h3_cells_time_series\n    ...\n\n\nkedro run --pipeline fetch_measures_on_grids --env production\n\n\nParlons maintenant du déploiement en production, et vous allez voir que c’est là que la puissance du framework Kedro prend tout son sens. Comme vous pouvez le voir à l’écran, le passage de l’environnement de test à la production se fait de manière élégante et simple grâce à une simple modification des fichiers de configuration. Concrètement :\nEn environnement de test, nous lisons et écrivons nos données dans des fichiers locaux simples (JSON, CSV) En production, nous conservons exactement la même logique de pipeline, mais nous connectons nos données à Kafka via ConfluentKafkaAvroDataset, qui est une implémentation personnalisée de Dataset, qui nous permet d’écrire dans Kafka tout en garantissant la cohérence des données via Schema Registry.\nCette approche présente plusieurs avantages :\nLe code reste strictement identique entre les environnements Seule la configuration change, ce qui réduit les risques d’erreur Les tests peuvent être effectués en local avec des fichiers plats"
  },
  {
    "objectID": "presentation.html#la-perspective-du-manager-et-de-lingénieur",
    "href": "presentation.html#la-perspective-du-manager-et-de-lingénieur",
    "title": "Python & kedropourl’agriculture de précision",
    "section": "La perspective du manager et de l’ingénieur",
    "text": "La perspective du manager et de l’ingénieur\n\n\nstructure rigide : expérience de développement excellente\n\nPoC à la production en un temps relativement court\n\n\n\n\n\nréactivité de la communauté : Slack avec 2200 inscrit·es\n\n\n\n\nvers un pipeline agnostique\n\ncode des node dépend encore des libraries : pandas/polars/spark\nnouvelles librairies pour palier cette dépendance : ibis, narwhal"
  },
  {
    "objectID": "presentation.html#lensemble-du-tuyau",
    "href": "presentation.html#lensemble-du-tuyau",
    "title": "Python & kedropourl’agriculture de précision",
    "section": "L’ensemble du tuyau",
    "text": "L’ensemble du tuyau"
  }
]