---
title: "Python & kedro<br>pour<br>l'agriculture de précision"
# subtitle: "Étude de cas Inclusive Brains"
author: "Paul Arnaud & Flavien Lambert<br>Data Engineering Team<br><img src='./images/sencrop_logo_vertical_RVB.png' height=100px>"
format:
  revealjs:
    title-block-banner: true
    title-slide-attributes: 
      data-notes: Bonjour à toutes et à tous, c'est notre première participation à Pycon France en tant qu'orateurs et nous tenons à remercier les organistrices et organisateurs de nous donner cette opportunité. Paul et moi faisons partie de la petite équipe de data engineering de Sencrop dont nous allons vous parler dans une minute. Après la présentation du problème que nous voulions résoudre, nous introduirons l'outil python que nous avons utilisé pour cela et nous montrerons comment cela nous a permis de passer relativement facilement en production.
    width: 1244
    toc: true
    toc-depth: 1
    toc-title: Agenda
    theme: [custom.scss]
    menu: false
    progress: false
    scrollable: true
    highlight-style: github
    code-line-numbers: false
    slide-number: true
    show-slide-number: all
    gfm:
      mermaid-format: js
---

# L'agtech au pays de la patate

## Sencrop {background-opacity=0.25 background-image="./images/raincrop.jpg"}

- 35000 stations météorologiques réparties sur toute l'Europe

:::{.columns}
:::{.column #vcenter}

```{=html}
<div class="container">
  <img src="./images/grey-temp-hum-o.png" width="70em"/>
  <p>température de l'air & hygrométrie</p>
  <img src="./images/grey-wind.png" width="70em"/>
  <p>direction et vitesse du vent</p>
  <img src="./images/grey-rain-o.png" width="70em"/>
  <p>pluviométrie</p>
  <img src="./images/grey-dew-point.png" width="70em"/>
  <p>point de rosée</p>
</div>
```

:::
:::{.column #vcenter}
:::{.fragment .fade-in}

![](./images/stations-h3-7.png)

:::
:::
:::

## Spatialisation 

- Est-ce que l'on peut fournir de la donnée météorologique de qualité sur n'importe quelle localisation sur le territoire ?

:::{.fragment .fade-in}
:::{.columns}

:::{.column #vcenter}

- utilisation de maillage `h3` : index spatial hiérarchique
- comparaison des mesures de stations avec les médianes sur les mailles

:::
:::{.column #vcenter}

![](./images/pentagon_hexagon_children.png)

:::
:::
:::

# Un, deux, trois... <span style="color:yellow">◆</span> kedro!

## kedro en trois mots

:::{.fragment}

- librairie de transformations de données
  - découplage entre les sources de données et les transformations
:::

:::{.fragment}
:::{.columns}
:::{.column #vcenter}

```{mermaid}
flowchart LR
	node2[node]
	node7[node]
	
	input1[(dataset)]
	input2[(dataset)]
	output[(dataset)]
	
  subgraph pipeline[pipeline]
    direction LR
  	node2 --> node7
  	%% node3 --> node5 --> node7
  	input1 --> node2
  	input2 --> node2
  	%% input3 --> node5
  	node7 --> output
  end

  style pipeline color:#000
```

:::
:::{.column #vcenter}
<!-- :::{.fragment} -->

- `node`
  - fonction — au sens Python — avec un/des `dataset` d'entrée/sortie
- `pipeline`
  - séquence de `node`
- `catalog`
  - un ensemble de `dataset`

:::
:::
:::

:::{.notes}
kedro est une librairie open-source qui a été initialement développée par Quantum Black, la branche AI de Mc Kinsey (probablement
financé par le contribuable français) qui a été depuis transmise à la Linux Foundation. Je devais être destiné à utiliser kedro parce
que, comme les trous noirs quantiques, je n'ai pas de cheveux.

kedro permet de construire efficacement des pipelines de transformations de données et se base sur trois concepts qui vous sont
peut-être familiers : des nœuds, des pipelines et un catalogue dont nous allons expliquer la signification dans quelques instants.
:::

## pipelines & nodes

:::: {.columns}
:::{.column #vcenter}

```{mermaid}
flowchart TB
	node2[map_locations_to_grids]
	node7[extract_unique_grid_ids]
	
	input1[(locations)]
	input2[h3-grid-resolution]
	output[(unique-grid-ids)]
	
  subgraph pipeline[map_locations_to_grids]
    direction TB
  	node2 --> node7
  	%% node3 --> node5 --> node7
  	input1 --> node2
  	input2 --> node2
  	%% input3 --> node5
  	node7 --> output
  end

  style pipeline color:#000
```

:::
:::{.column #vcenter}
:::{.r-stack}
:::{.fragment .fade-out}

```{.python filename="/src/pipelines/map_locations_to_grids/pipeline.py"}
from typing import Any
from kedro.pipeline import Pipeline, node
from .nodes import map_locations_to_grids, extract_unique_grid_ids


def create_pipeline(**kwargs: Any) -> Pipeline:
    return Pipeline(
        nodes=[
            node(
                map_locations_to_grids,
                inputs={
                    "locations": "locations",
                    "resolution": "params:h3-grid-resolution",
                },
                outputs="grids",
            ),
            node(
                extract_unique_grid_ids,
                inputs={"grids": "grids"},
                outputs="unique-grid-ids",
            )
        ],
    )
```

:::
:::{.fragment .fade-in #vcenter}

```{.python filename="/src/pipelines/map_locations_to_grids/nodes.py"}
import h3
import pandas


def map_locations_to_grids(
    locations: pandas.DataFrame, resolution: int
) -> pandas.DataFrame:
    return pandas.DataFrame(
        data=[
            {
                "location_id": location["id"],
                "grid_id": h3.geo_to_h3(
                    lat=location["latitude"],
                    lng=location["longitude"],
                    resolution=resolution,
                ),
            }
            for location in locations.to_dict(orient="records")
        ]
    )

def extract_unique_grid_ids(grids: pandas.DataFrame) -> pandas.DataFrame:
    return grids["grid_id"].unique()
```

:::
:::
:::
:::

:::{.notes}
Un nœud au sens de kedro est juste une fonction python qui peut avoir des entrées et des sorties. Elle est totalement
indépendante du reste et peut/doit donc être testée de manière unitaire.

Un pipeline est constitué de séquences de nœuds. La structure du DAG, ce qui en fait un pipeline, ce sont les entrées/sorties.
Si la sortie d'un nœud fait partie des entrées d'un autre alors ces deux nœuds seront exécutés séquentiellement.

Vous remarquerez que les entrées/sorties sont définies par des chaînes de caractères, des noms de datasets. Que représentent-ils
et comment passe-t-on d'une chaîne de caractère à un DataFrame pandas comme dans l'exemple. Et bien, c'est le catalogue !
:::

## catalog & environment

- `catalog` : définition des `dataset` d'entrée et de sortie
- `environment` : ensemble du `catalog` et d'un jeu de paramètres

:::{.columns}
:::{.column}

```{.yaml filename=/conf/test-local/catalog.yaml }
locations:
  type: pandas.JSONDataset
  filepath: data/01_raw/test_local/two_locations.json

grids:
  type: pandas.JSONDataset
  filepath: data/02_intermediate/grids.json
```

<br>

```sh
kedro run --pipeline map_locations_to_grids --env test-local
```

:::
:::{.column}
:::{.fragment .fade-in}

```{.yaml filename=/conf/production/catalog.yaml width="200px"}
locations:
  type: pandas.JSONDataset
  filepath: s3://virtual-stations/locations.json






```

<br>

```sh
kedro run --pipeline map_locations_to_grids --env production
```

:::
:::
:::

:::{.notes}
Le catalogue est une liste de tous les datasets qui sont matérialisés — c'est à dire pas seulement en mémoire. Ils se présentent
sous la forme d'un fichier yaml dont chaque clef principale correspond aux noms que vous trouvez dans les pipelines. L'élément le
plus important est le type, c'est lui va déterminer comment on passe d'un nom à une structure python. Un type de dataset est en fait
une classe Python qui implémente un certain nombre de méthode particulièrement, load et save. Lorsqu'un pipeline est exécuté,
avant l'exécution d'un nœud, kedro va utiliser la méthode load pour charger les données du dataset en mémoire et appliquer la
transformation définie dans le nœud. Le résultat est ensuite matérialisé si besoin par l'intermédiaire de la méthode save du dataset
de sortie.

Dans notre exemple, la dataset est un pandas.CSVDataset. C'est une classe qui utilise pandas pour lire un fichier CSV, qui ici, se trouve
dans un bucket s3. Vous remarquerez que le catalogue es défini dans un dossier conf/base. Pour exécuter le pipeline, vous utilisez la CLI
de kedro en précisant l'environnement, ici base. Et c'est ici que réside l'un des forces les plus tangibles de kedro. Vous avez parfaitement
remarqué que le pipeline est indépendant des sources de données, la seule chose importante est que le dataset renvoie dans notre cas un dataframe.
Je peux donc définir un autre catalogue avec un autre type de dataset qui va lui aussi me fournir une dataframe. Par exemple, je peux créer un
catalogue de fichiers CSV locaux et tester mon pipeline avec ces nouvelles sources !

Notez que tous ces datasets existent déjà, c'est une librairie appelée kedro-datasets qui est maintenue par les équipes de quantum black et la
communauté.
:::

## structure du projet

- séparation des sources de données et des pipelines

```{.bash code-line-numbers="4,15,17"}
.
├── conf
│   ├── base
│   │   ├── catalog.yml
│   │   └── parameters.yml
│   └── local
│       └── credentials.yml
├── data
│   ├── 01_raw
│   └── ...
├── notebooks
├── pyproject.toml
└── src
    └── my_project
        ├── pipeline_registry.py
        └── pipelines
            └── my_pipeline
                ├── nodes.py
                └── pipeline.py
```

:::{.notes}
Vous l'avez compris, kedro impose une séparation claire des datasets et des pipelines, des sources/sink et des transformations.
Cela se retrouve dans la structure d'un projet. Tous les catalogues se trouvent dans le dossier conf. Dans l'exemple
précédent, j'aurais créé le dossier test-local

Les pipelines sont dans le dossier pipelines et sont répertoriés dans le fichier registry.
:::

# Production, un YAML modifié, sérénité

## Dumas du tuyau : trois écrivains et un lecteur

:::{.r-stretch }
```{mermaid}
%%| fig-align: center
flowchart LR

	measures[(<br>mesures<br>des stations)]
	getstatistics[<br>calculs<br>des statistiques]

	meteoblue[(<br>meteoblue<br>API)]
  getforecasts[<br>récupération<br>des prévisions]
  gethistory[<br>récupération<br>de l'historique]

	forecasts[(<br>prévisions)]
	history[(<br>historique)]
	statistics[(<br>statistiques)]

	measures --> getstatistics --> statistics
	meteoblue --> getforecasts --> forecasts
	meteoblue --> gethistory --> history
	
	combine[<br>combinaison<br>des sources] --> timeseries[(<br>série temporelle<br>finale)]
  statistics --> combine
	forecasts --> combine
	history --> combine
```

:::

## Kafka sur le rivage

- kafka n'est pas le sujet mais c'est notre dataset de production !

:::{.columns}

:::{.column}

```{.yaml filename="/conf/test-measures/catalog.yml" code-line-numbers="1,3,5,6"}
locations:
  type: pandas.JSONDataset
  filepath: data/01_raw/test_measures/locations.json

formatted_measures_on_grids:
  type: pandas.CSVDataset
  filepath: data/03_primary/test_measures/measures_on_grids.csv
```

:::
:::{.column}

```{.yaml filename="/conf/production/catalog.yml" code-line-numbers="1,3,5,6"}
locations:
  type: pandas.JSONDataset
  filepath: s3://virtual-stations/virtual-stations.json

formatted_measures_on_grids:
  type: datasets.ConfluentKafkaAvroDataset
  save_args:
    topic: h3_cells_time_series
    ...
```

:::
:::

# Vers des pipelines tout-terrain ?

## La perspective du manager et de l'ingénieur

:::{.fragment}

- structure rigide : expérience de développement excellente
  - PoC à la production en un temps relativement court

:::
:::{.fragment}

- réactivité de la communauté : Slack avec 2200 inscrit·es

:::
:::{.fragment}

- vers un pipeline agnostique
  - code des `node` dépend encore des libraries : `pandas`/`polars`/`spark`
  - nouvelles librairies pour palier cette dépendance : [`ibis`](https://ibis-project.org/), [`narwhal`](https://github.com/narwhals-dev/narwhals)
:::

# {background-opacity=0.25 background-image="./images/raincrop.jpg"}

:::{.r-fit-text}

Merci de votre attention !

:::

## L'ensemble du tuyau

![](./images/virtual-stations.png){fig-align="center"}