---
title: "Python & kedro<br>pour<br>l'agriculture de précision"
# subtitle: "Étude de cas Inclusive Brains"
author: "Paul Arnaud & Flavien Lambert<br>Data Engineering Team<br><img src='./images/sencrop_logo_vertical_RVB.png' height=100px>"
format:
  revealjs:
    title-block-banner: true
    width: 1244
    toc: true
    toc-depth: 1
    toc-title: agenda
    theme: [custom.scss]
    menu: false
    progress: false
    scrollable: true
    navigation-mode: grid
    highlight-style: github
    code-line-numbers: false
    mermaid:
      theme: forest
---
# l'agtech au pays de la patate

## sencrop {background-opacity=0.25 background-image="./images/raincrop.jpg"}

- 35000 stations météorologiques réparties sur toute l'Europe

```{=html}
<div class="container">
  <img src="./images/grey-temp-hum-o.png" width="70em"/>
  <p>température de l'air & hygrométrie</p>
  <img src="./images/grey-wind.png" width="70em"/>
  <p>direction et vitesse du vent</p>
  <img src="./images/grey-rain-o.png" width="70em"/>
  <p>pluviométrie</p>
  <img src="./images/grey-dew-point.png" width="70em"/>
  <p>point de rosée</p>
</div>
```

## spatialisation 

- Est-ce que l'on peut fournir de la donnée météorologique de qualité sur n'importe quelle localisation sur le territoire ?

:::{.columns}

:::{.column #vcenter}

- comparaison des mesures de stations avec les médianes sur les grilles `h3`

:::
:::{.column #vcenter}

![](./images/pentagon_hexagon_children.png)

:::
:::

# un, deux, trois... <span style="color:yellow">◆</span> kedro!

## kedro en trois mots

:::{.fragment}

- librairie de transformations de données
  - découplage entre les sources de données et les transformations
:::

:::{.fragment}

- trois concepts principaux
  - `node` : fonction — au sens Python — avec un/des `dataset` d'entrée et un/des `dataset` de sortie
  - `pipeline` : Direct Acyclic Graph composé de `node`
  - `catalog` : un ensemble de `dataset`

:::

## pipelines & nodes

:::: {.columns}
:::{.column}

```{.python filename="/src/pipelines/my_pipeline/pipeline.py"}
from kedro.pipeline import Pipeline, node, pipeline

from .nodes import create_model, preprocess


def create_pipeline(**kwargs) -> Pipeline:
    return pipeline(
        [
            node(
                func=preprocess,
                inputs="companies",
                outputs="preprocessed_companies",
            ),
            ...
            node(
                func=create_model,
                inputs=["preprocessed_companies", ...],
                outputs=...,
            ),
        ]
    )
```

:::
:::{.column}

```{.python filename="/src/pipelines/my_pipeline/nodes.py"}
import pandas as pd

...

def preprocess(data: pd.DataFrame) -> pd.DataFrame:
    ...
    return result
```

:::
:::

## catalog & environment

- `catalog` : définition des `dataset` d'entrée et de sortie
- `environment` : ensemble du `catalog` et d'un jeu de paramètres

:::{.r-stack}
:::{.fragment .fade-in-then-out}

```{.yaml filename=/conf/base/catalog.yaml width="200px"}
companies:
  type: pandas.CSVDataset
  filepath: s3://01_raw/companies.csv

preprocessed_companies:
  type: pandas.ParquetDataset
  filepath: s3://02_intermediate/preprocessed_companies.pq
```

<br>

```sh
kedro run --pipeline my_pipeline --env base
```

:::

:::{.fragment .fade-in}

```{.yaml filename=/conf/test-local/catalog.yaml }
companies:
  type: pandas.CSVDataset
  filepath: data/01_raw/companies.csv

preprocessed_companies:
  type: pandas.CSVDataset
  filepath: data/02_intermediate/preprocessed_companies.csv
```

<br>

```sh
kedro run --pipeline my_pipeline --env test-local
```

:::
:::

## structure du projet

- séparation des sources de données et des pipelines

```{.bash code-line-numbers="4,15,17"}
.
├── conf
│   ├── base
│   │   ├── catalog.yml
│   │   └── parameters.yml
│   └── local
│       └── credentials.yml
├── data
│   ├── 01_raw
│   └── ...
├── notebooks
├── pyproject.toml
└── src
    └── my_project
        ├── pipeline_registry.py
        └── pipelines
            └── my_pipeline
                ├── nodes.py
                └── pipeline.py
```


# kedro: du développement à la production

## Dumas du tuyau : trois écrivains et un lecteur

## des tests à la production : une histoire de sources

:::{.columns}

:::{.column}

```{.yaml filename="/conf/test-measures/catalog.yml"}
locations:
  type: pandas.JSONDataset
  filepath: data/01_raw/test_measures/locations.json

formatted_measures_on_grids:
  type: pandas.CSVDataset
  filepath: data/03_primary/test_measures/measures_on_grids.csv
  load_args:
    parse_dates:
      - timestamp
```

:::
:::{.column}

```{.yaml filename="/conf/production/catalog.yml"}
locations:
  type: pandas.JSONDataset
  filepath: s3://virtual-stations/virtual-stations.json

formatted_measures_on_grids:
  type: datasets.ConfluentKafkaAvroDataset
  save_args:
    topic: h3_cells_time_series
    cluster:
      bootstrap_servers: ...
    schema_registry:
      url: ...
    schema:
      name: h3_cells_time_series
      type: record
      fields:
        - name: h3_cell_id
          type: string
        - name: h3_cell_resolution
          type: int
        ...
```

:::
:::

# vers des pipelines tout-terrain ?

## From Salamèche to Dracaufeu

```{mermaid}
flowchart LR
  subgraph AWS ["AWS Cloud"]
      A["fa:fa-clock MWAA - Airflow"] -->|Schedules hourly| B["fa:fa-tasks ECS Task"]
      subgraph B[ECS Task]
          subgraph C ["fa:fa-docker Docker Image"]
              D["fa:fa-cogs Kedro Framework"]
          end
      end
      subgraph F ["fa:fa-database Aurora PostgreSQL"]
          G["fa:fa-table Timeseries Table"]
      end
  end

  subgraph Confluent ["Confluent Cloud"]
      E["fa:fa-stream Kafka Topic"]
  end

  D -->|Produces data| E
  E -->|Buffers data| F

  style A fill:#6a4c93,color:#fff,stroke:#333,stroke-width:2px,rounded
  style B fill:#1982c4,color:#fff,stroke:#333,stroke-width:2px,rounded
  style C fill:#8ac926,color:#333,stroke:#333,stroke-width:2px,rounded
  style D fill:#ffca3a,color:#333,stroke:#333,stroke-width:2px,rounded
  style E fill:#ff595e,color:#fff,stroke:#333,stroke-width:2px,rounded
  style F fill:#1982c4,color:#fff,stroke:#333,stroke-width:2px,rounded
  style G fill:#6a4c93,color:#fff,stroke:#333,stroke-width:2px,rounded
  style AWS fill:#f8f9fa,stroke:#1982c4,stroke-width:2px,rounded
  style Confluent fill:#fff0f3,stroke:#ff595e,stroke-width:2px,rounded
```


# vers l'agnoticisme ?

## la perspective du manager

:::{.fragment}

- structure rigide : expérience de développement excellente
  - PoC à la production en un temps relativement court

:::
:::{.fragment}

- réactivité de la communauté : Slack avec 2200 inscrit·es

:::
:::{.fragment}

- vers un pipeline agnostique
  - code des `node` dépend encore des libraries : `pandas`/`polars`/`spark`
  - nouvelles librairies pour palier cette dépendance : [`ibis`](https://ibis-project.org/), [`narwhal`](https://github.com/narwhals-dev/narwhals)
:::
