---
title: "Python & kedro<br>pour<br>l'agriculture de précision"
# subtitle: "Étude de cas Inclusive Brains"
author: "Paul Arnaud & Flavien Lambert<br>Data Engineering Team<br><img src='./images/sencrop_logo_vertical_RVB.png' height=100px>"
format:
  revealjs:
    title-block-banner: true
    title-slide-attributes: 
      data-notes: Bonjour à toutes et à tous, c'est notre première participation à Pycon France en tant qu'orateurs et nous tenons à remercier les organistrices et organisateurs de nous donner cette opportunité. Paul et moi faisons partie de la petite équipe de data engineering de Sencrop dont nous allons vous parler dans une minute. Après la présentation du problème que nous voulions résoudre, nous introduirons l'outil python que nous avons utilisé pour cela et nous montrerons comment cela nous a permis de passer relativement facilement en production.
    width: 1244
    toc: true
    toc-depth: 1
    toc-title: agenda
    theme: [custom.scss]
    menu: false
    progress: false
    scrollable: true
    navigation-mode: grid
    highlight-style: github
    code-line-numbers: false
    mermaid:
      theme: forest
---

# l'agtech au pays de la patate

## sencrop {background-opacity=0.25 background-image="./images/raincrop.jpg"}

- 35000 stations météorologiques réparties sur toute l'Europe

:::{.columns}
:::{.column #vcenter}

```{=html}
<div class="container">
  <img src="./images/grey-temp-hum-o.png" width="70em"/>
  <p>température de l'air & hygrométrie</p>
  <img src="./images/grey-wind.png" width="70em"/>
  <p>direction et vitesse du vent</p>
  <img src="./images/grey-rain-o.png" width="70em"/>
  <p>pluviométrie</p>
  <img src="./images/grey-dew-point.png" width="70em"/>
  <p>point de rosée</p>
</div>
```

:::
:::{.column #vcenter}
:::{.fragment .fade-in}

![](./images/stations-h3-7.png)

:::
:::
:::

## spatialisation 

- Est-ce que l'on peut fournir de la donnée météorologique de qualité sur n'importe quelle localisation sur le territoire ?

:::{.fragment .fade-in}
:::{.columns}

:::{.column #vcenter}

- comparaison des mesures de stations avec les médianes sur les grilles `h3`

:::
:::{.column #vcenter}

![](./images/pentagon_hexagon_children.png)

:::
:::
:::

# un, deux, trois... <span style="color:yellow">◆</span> kedro!

## kedro en trois mots

:::{.fragment}

- librairie de transformations de données
  - découplage entre les sources de données et les transformations
:::

:::{.fragment}

- trois concepts principaux
  - `node` : fonction — au sens Python — avec un/des `dataset` d'entrée et un/des `dataset` de sortie
  - `pipeline` : Direct Acyclic Graph composé de `node`
  - `catalog` : un ensemble de `dataset`

:::

:::{.notes}
kedro est une librairie open-source qui a été initialement développée par Quantum Black, la branche AI de Mc Kinsey (probablement
financé par le contribuable français) qui a été depuis transmise à la Linux Foundation. Je devais être destiné à utiliser kedro parce
que, comme les trous noirs quantiques, je n'ai pas de cheveux.

kedro permet de construire efficacement des pipelines de transformations de données et se base sur trois concepts qui vous sont
peut-être familiers : des nœuds, des pipelines et un catalogue dont nous allons expliquer la signification dans quelques instants.
:::

## pipelines & nodes

:::: {.columns}
:::{.column}

```{.python filename="/src/pipelines/my_pipeline/pipeline.py"}
from kedro.pipeline import Pipeline, node, pipeline

from .nodes import create_model, preprocess


def create_pipeline(**kwargs) -> Pipeline:
    return pipeline(
        [
            node(
                func=preprocess,
                inputs="companies",
                outputs="preprocessed_companies",
            ),
            ...
            node(
                func=create_model,
                inputs=["preprocessed_companies", ...],
                outputs=...,
            ),
        ]
    )
```

:::
:::{.column}

```{.python filename="/src/pipelines/my_pipeline/nodes.py"}
import pandas as pd

...

def preprocess(data: pd.DataFrame) -> pd.DataFrame:
    ...
    return result
```

:::
:::

:::{.notes}
Un nœud au sens de kedro est juste une fonction python qui peut avoir des entrées et des sorties. Elle est totalement
indépendante du reste et peut/doit donc être testée de manière unitaire.

Un pipeline est constitué de séquences de nœuds. La structure du DAG, ce qui en fait un pipeline, ce sont les entrées/sorties.
Si la sortie d'un nœud fait partie des entrées d'un autre alors ces deux nœuds seront exécutés séquentiellement.

Vous remarquerez que les entrées/sorties sont définies par des chaînes de caractères, des noms de datasets. Que représentent-ils
et comment passe-t-on d'une chaîne de caractère à un DataFrame pandas comme dans l'exemple. Et bien, c'est le catalogue !
:::

## catalog & environment

- `catalog` : définition des `dataset` d'entrée et de sortie
- `environment` : ensemble du `catalog` et d'un jeu de paramètres

:::{.r-stack}
:::{.fragment .fade-in-then-out}

```{.yaml filename=/conf/base/catalog.yaml width="200px"}
companies:
  type: pandas.CSVDataset
  filepath: s3://01_raw/companies.csv

preprocessed_companies:
  type: pandas.ParquetDataset
  filepath: s3://02_intermediate/preprocessed_companies.pq
```

<br>

```sh
kedro run --pipeline my_pipeline --env base
```

:::

:::{.fragment .fade-in}

```{.yaml filename=/conf/test-local/catalog.yaml }
companies:
  type: pandas.CSVDataset
  filepath: data/01_raw/companies.csv

preprocessed_companies:
  type: pandas.CSVDataset
  filepath: data/02_intermediate/preprocessed_companies.csv
```

<br>

```sh
kedro run --pipeline my_pipeline --env test-local
```

:::{.notes}
Le catalogue est une liste de tous les datasets qui sont matérialisés — c'est à dire pas seulement en mémoire. Ils se présentent
sous la forme d'un fichier yaml dont chaque clef principale correspond aux noms que vous trouvez dans les pipelines. L'élément le
plus important est le type, c'est lui va déterminer comment on passe d'un nom à une structure python. Un type de dataset est en fait
une classe Python qui implémente un certain nombre de méthode particulièrement, load et save. Lorsqu'un pipeline est exécuté,
avant l'exécution d'un nœud, kedro va utiliser la méthode load pour charger les données du dataset en mémoire et appliquer la
transformation définie dans le nœud. Le résultat est ensuite matérialisé si besoin par l'intermédiaire de la méthode save du dataset
de sortie.

Dans notre exemple, la dataset est un pandas.CSVDataset. C'est une classe qui utilise pandas pour lire un fichier CSV, qui ici, se trouve
dans un bucket s3. Vous remarquerez que le catalogue es défini dans un dossier conf/base. Pour exécuter le pipeline, vous utilisez la CLI
de kedro en précisant l'environnement, ici base. Et c'est ici que réside l'un des forces les plus tangibles de kedro. Vous avez parfaitement
remarqué que le pipeline est indépendant des sources de données, la seule chose importante est que le dataset renvoie dans notre cas un dataframe.
Je peux donc définir un autre catalogue avec un autre type de dataset qui va lui aussi me fournir une dataframe. Par exemple, je peux créer un
catalogue de fichiers CSV locaux et tester mon pipeline avec ces nouvelles sources !

Notez que tous ces datasets existent déjà, c'est une librairie appelée kedro-datasets qui est maintenue par les équipes de quantum black et la
communauté.
:::

:::
:::

## structure du projet

- séparation des sources de données et des pipelines

```{.bash code-line-numbers="4,15,17"}
.
├── conf
│   ├── base
│   │   ├── catalog.yml
│   │   └── parameters.yml
│   └── local
│       └── credentials.yml
├── data
│   ├── 01_raw
│   └── ...
├── notebooks
├── pyproject.toml
└── src
    └── my_project
        ├── pipeline_registry.py
        └── pipelines
            └── my_pipeline
                ├── nodes.py
                └── pipeline.py
```

:::{.notes}
Vous l'avez compris, kedro impose une séparation claire des datasets et des pipelines, des sources/sink et des transformations.
Cela se retrouve dans la structure d'un projet. Tous les catalogues se trouvent dans le dossier conf. Dans l'exemple
précédent, j'aurais créé le dossier test-local

Les pipelines sont dans le dossier pipelines et sont répertoriés dans le fichier registry.
:::

# kedro: du développement à la production

## Dumas du tuyau : trois écrivains et un lecteur

## à un YAML de la production...

:::{.columns}

:::{.column}

```{.yaml filename="/conf/test-measures/catalog.yml"}
locations:
  type: pandas.JSONDataset
  filepath: data/01_raw/test_measures/locations.json

formatted_measures_on_grids:
  type: pandas.CSVDataset
  filepath: data/03_primary/test_measures/measures_on_grids.csv
  load_args:
    parse_dates:
      - timestamp
```

:::
:::{.column}

```{.yaml filename="/conf/production/catalog.yml"}
locations:
  type: pandas.JSONDataset
  filepath: s3://virtual-stations/virtual-stations.json

formatted_measures_on_grids:
  type: datasets.ConfluentKafkaAvroDataset
  save_args:
    topic: h3_cells_time_series
    cluster:
      bootstrap_servers: ...
    schema_registry:
      url: ...
    schema:
      name: h3_cells_time_series
      type: record
      fields:
        - name: h3_cell_id
          type: string
        - name: h3_cell_resolution
          type: int
        ...
```

:::
:::

## ...enfin presque

![](./images/virtual-stations.png){fig-align="center"}

# vers des pipelines tout-terrain ?

## la perspective du manager et de l'ingénieur

:::{.fragment}

- structure rigide : expérience de développement excellente
  - PoC à la production en un temps relativement court

:::
:::{.fragment}

- réactivité de la communauté : Slack avec 2200 inscrit·es

:::
:::{.fragment}

- vers un pipeline agnostique
  - code des `node` dépend encore des libraries : `pandas`/`polars`/`spark`
  - nouvelles librairies pour palier cette dépendance : [`ibis`](https://ibis-project.org/), [`narwhal`](https://github.com/narwhals-dev/narwhals)
:::
