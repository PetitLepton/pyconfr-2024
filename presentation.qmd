---
title: "Python & kedro<br>pour<br>l'agriculture de précision"
# subtitle: "Étude de cas Inclusive Brains"
author: "Paul Arnaud & Flavien Lambert<br>Data Engineering Team<br><img src='./images/sencrop_logo_vertical_RVB.png' height=100px>"
format:
  revealjs:
    title-block-banner: true
    title-slide-attributes: 
      data-notes: Bonjour à toutes et à tous, c'est notre première participation à Pycon France en tant qu'orateurs et nous tenons à remercier les organistrices et organisateurs de nous donner cette opportunité. Paul et moi faisons partie de la petite équipe de data engineering de Sencrop dont nous allons vous parler dans une minute. Après la présentation du problème que nous voulions résoudre, nous introduirons l'outil python que nous avons utilisé pour cela et nous montrerons comment cela nous a permis de passer relativement facilement en production.
    width: 1244
    toc: true
    toc-depth: 1
    toc-title: Agenda
    theme: [custom.scss]
    menu: false
    progress: false
    scrollable: true
    highlight-style: github
    code-line-numbers: false
    slide-number: true
    show-slide-number: all
    gfm:
      mermaid-format: js
---

# L'agtech au pays de la patate

:::{.notes}
Pour commencer cette présentation j'aimerais vous parler un peu de Sencrop pour introduire les différentes problématiques auxquelles nous sommes confrontées aujourd'hui.
:::


## Sencrop {background-opacity=0.25 background-image="./images/raincrop.jpg"}

- 35000 stations météorologiques réparties sur toute l'Europe

:::{.columns}
:::{.column #vcenter}

```{=html}
<div class="container">
  <img src="./images/grey-temp-hum-o.png" width="70em"/>
  <p>température de l'air & hygrométrie</p>
  <img src="./images/grey-wind.png" width="70em"/>
  <p>direction et vitesse du vent</p>
  <img src="./images/grey-rain-o.png" width="70em"/>
  <p>pluviométrie</p>
  <img src="./images/grey-dew-point.png" width="70em"/>
  <p>point de rosée</p>
</div>
```

:::
:::{.column #vcenter}
:::{.fragment .fade-in}

![](./images/stations-h3-7.png)

:::
:::
:::

:::{.notes}
Sencrop c'est une startup de l'agtech fondée en 2016 dans le nord de la France, à Lille très exactement. Avec plus de 80 collaborateurs, 
Sencrop est le leader européen des stations météos connectées en couvrant l'Europe de l'ouest de plus de 35 000 capteurs. 

Le principe est simple, nos utilisateurs (qui sont agriculteurs la plupart du temps) peuvent installer une station dans leur champ. 
Ces stations possèdent plusieurs capteurs comme vous pouvez le voir sur l'écran juste ici. 
Les mesures apparaissent ensuite sur une application sur téléphone ou ordinateur.
Ces données météorologiques permettent à nos utilisateurs de prendre des décisions sur leur opération quotidienne. 
Ainsi, nos agriculteurs réussissent à optimiser leur l'irrigation afin de gagner certains volumes d'eau, 
mais également optimiser leur pulvérisation de produit phytosanitaire (qui coute très cher). 
Du côté agronomique, nous arrivons aussi à fournir de précieux conseils sur les périodes de risques maladies (pour protéger au mieux du mildiou par exemple) ou les périodes de gel (qui touchent régulierement nos vignerons).
:::

## Spatialisation 

- Est-ce que l'on peut fournir de la donnée météorologique de qualité sur n'importe quelle localisation sur le territoire ?

:::{.fragment .fade-in}
:::{.columns}

:::{.column #vcenter}

- utilisation de maillage `h3` : index spatial hiérarchique
- comparaison des mesures de stations avec les médianes sur les mailles

:::
:::{.column #vcenter}

![](./images/pentagon_hexagon_children.png)

:::
:::
:::

:::{.notes}
Aujourd'hui Sencrop est présente dans plus de 10 pays à travers l'Europe, principale autour de la France. 
Malgré notre large réseau de stations, nous faisons face à un défi majeur : la couverture géographique n'est pas uniforme. 
Certaines régions françaises, par exemple, sont moins densément équipées que d'autres. 
Cette situation pose un problème crucial : comment fournir des données météorologiques précises pour n'importe quel point GPS, même en l'absence de station à proximité immédiate ?
C'est là qu'intervient notre solution de génération de séries temporelles. 
L'objectif est ambitieux : créer un système capable de produire des données météorologiques fiables pour n'importe quelle coordonnée géographique en Europe de l'Ouest.

Pour relever ce défi de spatialisation des données, nous nous appuyons sur H3, le système de géo-indexation hiérarchique développé par Uber. Ce choix n'est pas anodin.
H3 divise la surface terrestre en hexagones de taille uniforme. Ce qui rend H3 particulièrement pertinent pour notre usage, c'est que :

- Les hexagones minimisent la distorsion des distances entre le centre et les bords, contrairement aux carrés
- Le système est hiérarchique avec 16 résolutions, allant d'hexagones de 1000km à moins d'un mètre de diamètre
- Chaque hexagone a un identifiant unique, ce qui facilite l'indexation et la recherche
- Les cellules voisines sont facilement identifiables grâce à l'algorithme de H3

Dans notre cas, nous utilisons principalement la résolution 7, qui correspond à des hexagones d'environ 5km de diamètre. Cette échelle nous permet de :

- Agréger efficacement les données de nos stations au sein d'une même cellule
- Créer une grille régulière pour l'interpolation des données
- Optimiser les performances de calcul tout en maintenant une précision pertinente pour l'agriculture

L'utilisation de H3 nous permet ainsi de structurer spatialement nos données météorologiques de manière efficace et d'appliquer nos algorithmes d'interpolation avec une plus grande confiance dans les résultats.
:::


# Un, deux, trois... <span style="color:yellow">◆</span> kedro!

## kedro en trois mots

:::{.fragment}

- librairie de transformations de données
  - découplage entre les sources de données et les transformations
:::

:::{.fragment}
:::{.columns}
:::{.column #vcenter}

```{mermaid}
flowchart LR
	node2[node]
	node7[node]
	
	input1[(dataset)]
	input2[(dataset)]
	output[(dataset)]
	
	input1 --> node2
	input2 --> node2

  subgraph pipeline[pipeline]
    direction LR
  	node2 --> node7
  	%% node3 --> node5 --> node7
  	%% input3 --> node5
  end

  node7 --> output

  style pipeline color:#000
```

:::
:::{.column #vcenter}
<!-- :::{.fragment} -->

- `node`
  - fonction — au sens Python — avec un/des `dataset` d'entrée/sortie
- `pipeline`
  - séquence de `node`
- `catalog`
  - un ensemble de `dataset`

:::
:::
:::

:::{.notes}
kedro est une librairie open-source qui a été initialement développée par Quantum Black, la branche AI de Mc Kinsey (probablement
financé par le contribuable français) qui a été depuis transmise à la Linux Foundation. Je devais être destiné à utiliser kedro parce
que, comme les trous noirs quantiques, je n'ai pas de cheveux.

kedro permet de construire efficacement des pipelines de transformations de données. Pour celleeux qui ne seraient pas familières avec
la notion de pipeline, le schéma de gauche en présente un simplifié. Le pipeline est ici très simple, il contient deux noeuds séquentiels
qui sont alimentés en amont par deux jeux de données et qui résultent la création d'un troisième.

kedro reprend, mot pour mot, ces trois concepts: des nœuds, des pipelines et un catalogue dont nous allons expliquer la signification
dans quelques instants.
:::

## pipelines & nodes

:::: {.columns}
:::{.column #vcenter}

```{mermaid}
flowchart TB
	node2[map_locations_to_grids]
	node7[extract_unique_grid_ids]
	
	input1[(locations)]
	input2[h3-grid-resolution]
	output[(unique-grid-ids)]
	
  subgraph pipeline[map_locations_to_grids]
    direction TB
  	node2 --> node7
  	%% node3 --> node5 --> node7
  	input1 --> node2
  	input2 --> node2
  	%% input3 --> node5
  	node7 --> output
  end

  style pipeline color:#000
```

:::
:::{.column #vcenter}
:::{.r-stack}
:::{.fragment .fade-out}

```{.python filename="/src/pipelines/map_locations_to_grids/pipeline.py"}
from typing import Any
from kedro.pipeline import Pipeline, node
from .nodes import map_locations_to_grids, extract_unique_grid_ids


def create_pipeline(**kwargs: Any) -> Pipeline:
    return Pipeline(
        nodes=[
            node(
                map_locations_to_grids,
                inputs={
                    "locations": "locations",
                    "resolution": "params:h3-grid-resolution",
                },
                outputs="grids",
            ),
            node(
                extract_unique_grid_ids,
                inputs={"grids": "grids"},
                outputs="unique-grid-ids",
            )
        ],
    )
```

:::
:::{.fragment .fade-in #vcenter}

```{.python filename="/src/pipelines/map_locations_to_grids/nodes.py"}
import h3
import pandas


def map_locations_to_grids(
    locations: pandas.DataFrame, resolution: int
) -> pandas.DataFrame:
    return pandas.DataFrame(
        data=[
            {
                "location_id": location["id"],
                "grid_id": h3.geo_to_h3(
                    lat=location["latitude"],
                    lng=location["longitude"],
                    resolution=resolution,
                ),
            }
            for location in locations.to_dict(orient="records")
        ]
    )

def extract_unique_grid_ids(grids: pandas.DataFrame) -> pandas.DataFrame:
    return grids["grid_id"].unique()
```

:::
:::
:::
:::

:::{.notes}

Un pipeline est une class Python constituée de séquences de node. La structure du DAG, ce qui en fait un pipeline, ce sont les entrées/sorties.
Si la sortie d'un nœud fait partie des entrées d'un autre alors ces deux nœuds seront exécutés séquentiellement. Dans l'exemple
à gauche, le nœud map_locations_to_grids intervient avant extract_unique_grid_ids parce que sa sortie grids est le jeud de donnée
d'entrée d'extract_unique_grid_ids.

Un nœud au sens de kedro juste une fonction python qui peut avoir des entrées et des sorties. Elle est totalement
indépendante du reste et peut/doit donc être testée de manière unitaire.

Vous remarquerez que les entrées/sorties sont définies par des chaînes de caractères, des noms de datasets. Que représentent-ils
et comment passe-t-on d'une chaîne de caractère à un DataFrame pandas comme dans l'exemple. Et bien, c'est le catalogue !
:::

## catalog & environment

- `catalog` : définition des `dataset` d'entrée et de sortie
- `environment` : ensemble du `catalog` et d'un jeu de paramètres

:::{.columns}
:::{.column}

```{.yaml filename=/conf/test-local/catalog.yaml }
locations:
  type: pandas.JSONDataset
  filepath: data/01_raw/test_local/two_locations.json

grids:
  type: pandas.JSONDataset
  filepath: data/02_intermediate/grids.json
```

<br>

```sh
kedro run --pipeline map_locations_to_grids --env test-local
```

:::
:::{.column}
:::{.fragment .fade-in}

```{.yaml filename=/conf/production/catalog.yaml width="200px"}
locations:
  type: pandas.JSONDataset
  filepath: s3://virtual-stations/locations.json






```

<br>

```sh
kedro run --pipeline map_locations_to_grids --env production
```

:::
:::
:::

:::{.notes}
Le catalogue est une liste de tous les datasets qui sont matérialisés — c'est à dire pas seulement en mémoire. Il se présente
sous la forme d'un fichier yaml dont chaque clef principale correspond aux noms que vous trouvez dans les pipelines. L'élément le
plus important est le type, c'est lui va déterminer comment on passe d'un nom à une structure python. Un type de dataset est en fait
une classe Python qui implémente un certain nombre de méthodes, notamment load et save. Lorsqu'un pipeline est exécuté,
avant l'exécution d'un nœud, kedro va utiliser la méthode load pour charger les données du dataset en mémoire et appliquer la
transformation définie dans le nœud. Le résultat est ensuite matérialisé si besoin par l'intermédiaire de la méthode save du dataset
de sortie.

Dans notre exemple, la dataset est un pandas.JSONDataset. C'est une classe qui utilise pandas pour lire un fichier JSON, qui ici, se trouve localement
dans un dossier. Vous remarquerez que le catalogue es défini dans un dossier conf/test-local. Pour exécuter le pipeline, vous utilisez la CLI
de kedro en précisant l'environnement, c'est-à-dire le nom du sous-dossier, ici test-local. Et c'est ici que réside l'un des forces les plus tangibles 
de kedro. Vous avez remarqué que le pipeline précédent est indépendant des sources de données, la seule chose importante est que le dataset renvoie dans 
notre cas un dataframe. Je peux donc définir un autre catalogue avec un autre type de dataset qui va lui aussi me fournir une dataframe. Par exemple, je peux créer un
catalogue avec un fichier JSON dans un bucket s3 et jouer mon pipeline avec ce nouvel environnement !

Notez que tous ces datasets existent déjà, c'est une librairie appelée kedro-datasets qui est maintenue par les équipes de quantum black et la
communauté.
:::

## structure du projet

- séparation des sources de données et des pipelines

```{.bash code-line-numbers="4,15,17"}
.
├── conf
│   ├── base
│   │   ├── catalog.yml
│   │   └── parameters.yml
│   └── local
│       └── credentials.yml
├── data
│   ├── 01_raw
│   └── ...
├── notebooks
├── pyproject.toml
└── src
    └── my_project
        ├── pipeline_registry.py
        └── pipelines
            └── my_pipeline
                ├── nodes.py
                └── pipeline.py
```

:::{.notes}
Vous l'avez compris, kedro impose une séparation claire des datasets et des pipelines, des sources/sink et des transformations.
Cela se retrouve dans la structure d'un projet. Tous les catalogues se trouvent dans le dossier conf. Dans l'exemple
précédent, j'aurais créé le dossier test-local

Les pipelines sont dans le dossier pipelines et sont répertoriés dans le fichier registry.

L'avantage de cette structure fixe est qu'elle permet à une personne qui connaît un peu kedro de voyager dans le projet facilement.
:::

# Production, un YAML modifié, sérénité

## Dumas du tuyau : trois écrivains et un lecteur

:::{.r-stretch }
```{mermaid}
%%| fig-align: center
flowchart LR

	measures[(<br>mesures<br>des stations)]
	getstatistics[<br>calculs<br>des statistiques]

	meteoblue[(<br>meteoblue<br>API)]
  getforecasts[<br>récupération<br>des prévisions]
  gethistory[<br>récupération<br>de l'historique]

	forecasts[(<br>prévisions)]
	history[(<br>historique)]
	statistics[(<br>statistiques)]

	measures --> getstatistics --> statistics
	meteoblue --> getforecasts --> forecasts
	meteoblue --> gethistory --> history
	
	combine[<br>combinaison<br>des sources] --> timeseries[(<br>série temporelle<br>finale)]
  statistics --> combine
	forecasts --> combine
	history --> combine
```

:::

:::{.notes}
Pour générer des séries temporelles précises pour n'importe quel point GPS, nous avons mis en place une architecture qui combine plusieurs sources de données. 
Laissez-moi vous expliquer notre pipeline de données.
Tout d'abord, nous exploitons deux sources principales :

Nos stations Sencrop :

Nous collectons en temps réel les mesures de nos stations déployées sur le terrain
Ces données brutes sont traitées pour calculer des statistiques pertinentes (principalement des médianes)


L'API Meteoblue :
Cette source externe nous fournit deux types de données :

Des prévisions météorologiques pour compléter nos données en temps réel
Un historique météorologique pour le passé

L'étape cruciale de notre système est la combinaison de ces sources. Notre algorithme fusionne intelligemment :

Les statistiques issues de nos stations
Les prévisions Meteoblue
L'historique Meteoblue

Cette approche multi-sources nous permet de générer une série temporelle finale qui bénéficie :

De la précision de nos mesures terrain là où nous avons des stations
De la couverture géographique complète offerte par Meteoblue
D'une continuité temporelle entre passé, présent et futur

Le résultat est une série temporelle cohérente et fiable, disponible pour n'importe quel point géographique de notre zone de couverture.
:::

:::{.notes}
Passage sur Kedro-viz
Laissez-moi maintenant vous présenter l'architecture de notre pipeline de données, visualisée ici grâce à Kedro-viz. 
Pour ceux qui ne connaissent pas, Kedro-viz est un outil fantastique qui nous permet de visualiser nos pipelines de données de manière claire et interactive, 
ce qui rend la compréhension de systèmes complexes beaucoup plus accessible.

Dans notre pipeline, tout commence avec les 'Locations' - ce sont les points GPS pour lesquels nous voulons générer des séries temporelles. 
Ces coordonnées constituent l'entrée principale de notre système.
Ces points GPS sont ensuite convertis en 'Grids' - il s'agit des fameuses cellules hexagonales H3 dont nous parlions précédemment. 
Cette étape de conversion est cruciale car elle nous permet de passer d'une logique de points GPS arbitraires à une structure spatiale régulière et optimisée.
Le pipeline se complexifie ensuite avec plusieurs étapes de traitement :

La récupération des séries temporelles sur les cellules H3 pour les différentes sources.

Ces traitements convergent vers la production de séries temporelles ordonnées ('Ordered Time Series On Grids'), qui sont ensuite affinées pour produire les séries temporelles finales.
Le point d'orgue de ce pipeline est la dernière étape : 'Time Series On Locations'. 
C'est ici que nous revenons à notre objectif initial - fournir des données météorologiques précises pour chaque point GPS demandé. 
Pour chaque localisation, nous stockons une série temporelle complète, combinant ainsi la puissance de notre infrastructure de données avec la précision géographique dont nos utilisateurs ont besoin.
:::

## Kafka sur le rivage

- kafka n'est pas le sujet mais c'est notre dataset de production !

:::{.columns}

:::{.column}

```{.yaml filename="/conf/test-measures/catalog.yml" code-line-numbers="1,3,5,6"}
locations:
  type: pandas.JSONDataset
  filepath: data/01_raw/test_measures/locations.json

formatted_measures_on_grids:
  type: pandas.CSVDataset
  filepath: data/03_primary/test_measures/measures_on_grids.csv

  
```

<br>

```sh
kedro run --pipeline fetch_measures_on_grids --env test-measures
```

:::
:::{.column}

```{.yaml filename="/conf/production/catalog.yml" code-line-numbers="1,3,5,6"}
locations:
  type: pandas.JSONDataset
  filepath: s3://virtual-stations/virtual-stations.json

formatted_measures_on_grids:
  type: datasets.ConfluentKafkaAvroDataset
  save_args:
    topic: h3_cells_time_series
    ...
```

<br>

```sh
kedro run --pipeline fetch_measures_on_grids --env production
```

:::
:::

:::{.notes}
Parlons maintenant du déploiement en production, et vous allez voir que c'est là que la puissance du framework Kedro prend tout son sens.
Comme vous pouvez le voir à l'écran, le passage de l'environnement de test à la production se fait de manière élégante et simple grâce à une simple modification des fichiers de configuration. 
Concrètement :

En environnement de test, nous lisons et écrivons nos données dans des fichiers locaux simples (JSON, CSV)
En production, nous conservons exactement la même logique de pipeline, mais nous connectons nos données à Kafka via ConfluentKafkaAvroDataset, 
qui est une implémentation personnalisée de Dataset, qui nous permet d'écrire dans Kafka tout en garantissant la cohérence des données via Schema Registry.

Cette approche présente plusieurs avantages :

Le code reste strictement identique entre les environnements
Seule la configuration change, ce qui réduit les risques d'erreur
Les tests peuvent être effectués en local avec des fichiers plats

:::

# Vers des pipelines tout-terrain ?

## La perspective du manager et de l'ingénieur

:::{.fragment}

- structure rigide : expérience de développement excellente
  - PoC à la production en un temps relativement court

:::
:::{.fragment}

- réactivité de la communauté : Slack avec 2200 inscrit·es

:::
:::{.fragment}

- vers un pipeline agnostique
  - code des `node` dépend encore des libraries : `pandas`/`polars`/`spark`
  - nouvelles librairies pour palier cette dépendance : [`ibis`](https://ibis-project.org/), [`narwhal`](https://github.com/narwhals-dev/narwhals)
:::

# {background-opacity=0.25 background-image="./images/raincrop.jpg"}

:::{.r-fit-text}

Merci de votre attention !

:::

## L'ensemble du tuyau

![](./images/virtual-stations.png){fig-align="center"}
